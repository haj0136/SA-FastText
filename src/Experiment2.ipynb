{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from skift import FirstColFtClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords dictionary\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_score(_score):\n",
    "    print(_score)\n",
    "    print(f\"Average Score: {np.mean(_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Words count: 10894\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "                                         SentimentText  Sentiment\n0                             wow... loved this place.          1\n1                                   crust is not good.          0\n2            not tasty and the texture was just nasty.          0\n3    stopped by during the late may bank holiday of...          1\n4    the selection on the menu was great and so wer...          1\n..                                                 ...        ...\n995  i think food should have flavor and texture an...          0\n996                           appetite instantly gone.          0\n997  overall i was not impressed and would not go b...          0\n998  the whole experience was underwhelming, and i ...          0\n999  then, as if i hadn't wasted enough of my life ...          0\n\n[1000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SentimentText</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wow... loved this place.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crust is not good.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>not tasty and the texture was just nasty.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>stopped by during the late may bank holiday of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the selection on the menu was great and so wer...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>i think food should have flavor and texture an...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>appetite instantly gone.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>overall i was not impressed and would not go b...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>the whole experience was underwhelming, and i ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>then, as if i hadn't wasted enough of my life ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "yelpData = pd.read_csv('../data/yelp_labelled.txt', sep='\\t', header=0, encoding=\"utf-8\")\n",
    "row_sizes = yelpData['SentimentText'].str.split().str.len()\n",
    "yelpData['SentimentText'] = yelpData['SentimentText'].str.lower()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "yelpData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = [word for word in text.split() if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_train(data, functions, word_ngrams=1):\n",
    "    _data = pd.DataFrame(data['SentimentText'])\n",
    "    for function in functions:\n",
    "        _data['SentimentText'] = _data['SentimentText'].apply(lambda x: function(x))\n",
    "    _row_sizes = _data['SentimentText'].str.split().str.len()\n",
    "    print(f\"Words count: {pd.Series.sum(_row_sizes)}\")\n",
    "    print(_data)\n",
    "    _sk_clf = FirstColFtClassifier(wordNgrams=word_ngrams, thread=1)  # lr=0.3, epoch=10\n",
    "    _scores = cross_val_score(_sk_clf, _data[['SentimentText']], data['Sentiment'], cv=5, scoring='accuracy')\n",
    "    print(f\"Words ngrams: {word_ngrams}\")\n",
    "    return _scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                         SentimentText\n",
      "0                                  wow... loved place.\n",
      "1                                          crust good.\n",
      "2                                 tasty texture nasty.\n",
      "3    stopped late may bank holiday rick steve recom...\n",
      "4                         selection menu great prices.\n",
      "..                                                 ...\n",
      "995                 think food flavor texture lacking.\n",
      "996                           appetite instantly gone.\n",
      "997                   overall impressed would go back.\n",
      "998  whole experience underwhelming, think we'll go...\n",
      "999  then, wasted enough life there, poured salt wo...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(yelpData, [remove_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.71  0.665 0.535 0.645 0.535]\n",
      "Average Score: 0.6180000000000001\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                         SentimentText\n",
      "0                                 wow loved this place\n",
      "1                                    crust is not good\n",
      "2             not tasty and the texture was just nasty\n",
      "3    stopped by during the late may bank holiday of...\n",
      "4    the selection on the menu was great and so wer...\n",
      "..                                                 ...\n",
      "995  i think food should have flavor and texture an...\n",
      "996                            appetite instantly gone\n",
      "997  overall i was not impressed and would not go back\n",
      "998  the whole experience was underwhelming and i t...\n",
      "999  then as if i hadnt wasted enough of my life th...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.605 0.63  0.575 0.62  0.565]\n",
      "Average Score: 0.599\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(yelpData, [remove_punctuation])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Words count: 10922\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "                                         SentimentText  Sentiment\n0                                 wow love this place           1\n1                                   crust be not good           0\n2             not tasty and the texture be just nasty           0\n3    stop by during the late may bank holiday off r...          1\n4    the selection on the menu be great and so be t...          1\n..                                                 ...        ...\n995  i think food should have flavor and texture an...          0\n996                             appetite instantly go           0\n997    overall i be not impress and would not go back           0\n998  the whole experience be underwhelm and i think...          0\n999  then as if i have+not waste enough of my life ...          0\n\n[1000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SentimentText</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wow love this place</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crust be not good</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>not tasty and the texture be just nasty</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>stop by during the late may bank holiday off r...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the selection on the menu be great and so be t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>i think food should have flavor and texture an...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>appetite instantly go</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>overall i be not impress and would not go back</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>the whole experience be underwhelm and i think...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>then as if i have+not waste enough of my life ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 21
    }
   ],
   "source": [
    "yelpDataLem = pd.read_csv('../data/YelpLemmatized.txt', sep='\\t', header=0, encoding=\"utf-8\")\n",
    "row_sizes = yelpDataLem['SentimentText'].str.split().str.len()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "yelpDataLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                         SentimentText\n",
      "0                                 wow love this place \n",
      "1                                   crust be not good \n",
      "2             not tasty and the texture be just nasty \n",
      "3    stop by during the late may bank holiday off r...\n",
      "4    the selection on the menu be great and so be t...\n",
      "..                                                 ...\n",
      "995  i think food should have flavor and texture an...\n",
      "996                             appetite instantly go \n",
      "997    overall i be not impress and would not go back \n",
      "998  the whole experience be underwhelm and i think...\n",
      "999  then as if i have+not waste enough of my life ...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.59  0.6   0.545 0.585 0.535]\n",
      "Average Score: 0.571\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(yelpDataLem, [])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words AND remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                         SentimentText\n",
      "0                                      wow loved place\n",
      "1                                           crust good\n",
      "2                                  tasty texture nasty\n",
      "3    stopped late may bank holiday rick steve recom...\n",
      "4                          selection menu great prices\n",
      "..                                                 ...\n",
      "995                  think food flavor texture lacking\n",
      "996                            appetite instantly gone\n",
      "997                    overall impressed would go back\n",
      "998  whole experience underwhelming think well go n...\n",
      "999  then wasted enough life there poured salt woun...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "[0.71  0.765 0.64  0.665 0.655]\n",
      "Average Score: 0.687\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(yelpData, [remove_stop_words, remove_punctuation])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words AND Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                         SentimentText\n",
      "0                                       wow love place\n",
      "1                                           crust good\n",
      "2                                  tasty texture nasty\n",
      "3    stop late may bank holiday rick steve recommen...\n",
      "4                           selection menu great price\n",
      "..                                                 ...\n",
      "995                     think food flavor texture lack\n",
      "996                              appetite instantly go\n",
      "997                      overall impress would go back\n",
      "998  whole experience underwhelm think wewill go ni...\n",
      "999  have+not waste enough life pour salt wound dra...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.715 0.72  0.56  0.645 0.62 ]\n",
      "Average Score: 0.652\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(yelpDataLem, [remove_stop_words])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-GRAMS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                         SentimentText\n",
      "0                                      wow loved place\n",
      "1                                           crust good\n",
      "2                                  tasty texture nasty\n",
      "3    stopped late may bank holiday rick steve recom...\n",
      "4                          selection menu great prices\n",
      "..                                                 ...\n",
      "995                  think food flavor texture lacking\n",
      "996                            appetite instantly gone\n",
      "997                    overall impressed would go back\n",
      "998  whole experience underwhelming think well go n...\n",
      "999  then wasted enough life there poured salt woun...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "[0.71  0.765 0.64  0.665 0.655]\n",
      "Average Score: 0.687\n",
      "                                         SentimentText\n",
      "0                                      wow loved place\n",
      "1                                           crust good\n",
      "2                                  tasty texture nasty\n",
      "3    stopped late may bank holiday rick steve recom...\n",
      "4                          selection menu great prices\n",
      "..                                                 ...\n",
      "995                  think food flavor texture lacking\n",
      "996                            appetite instantly gone\n",
      "997                    overall impressed would go back\n",
      "998  whole experience underwhelming think well go n...\n",
      "999  then wasted enough life there poured salt woun...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 2\n",
      "[0.64  0.725 0.53  0.63  0.57 ]\n",
      "Average Score: 0.619\n",
      "                                         SentimentText\n",
      "0                                      wow loved place\n",
      "1                                           crust good\n",
      "2                                  tasty texture nasty\n",
      "3    stopped late may bank holiday rick steve recom...\n",
      "4                          selection menu great prices\n",
      "..                                                 ...\n",
      "995                  think food flavor texture lacking\n",
      "996                            appetite instantly gone\n",
      "997                    overall impressed would go back\n",
      "998  whole experience underwhelming think well go n...\n",
      "999  then wasted enough life there poured salt woun...\n",
      "\n",
      "[1000 rows x 1 columns]\n",
      "Words ngrams: 3\n",
      "[0.685 0.665 0.545 0.635 0.615]\n",
      "Average Score: 0.6290000000000001\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in range(1, 4):  # word_ngrams\n",
    "    scores = preprocess_train(yelpData, [remove_stop_words, remove_punctuation], word_ngrams=i)\n",
    "    show_score(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Words count: 11557842\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "       Sentiment                                      SentimentText\n0              0  a bit of a disappointing film, i'd say: the ac...\n1              0  the acting was terrible, the cheesy, fake, che...\n2              1  plenty has been written about mamet's \"the hou...\n3              1  \"journey to the far side of the sun\" (aka \"dop...\n4              1  i lived in that area (hoboken and jersey city)...\n...          ...                                                ...\n49995          1  as a big dostoyevsky fan, i had always been di...\n49996          0  i didn't watch this show that much when i was ...\n49997          1  for people who are first timers in film making...\n49998          0  pumpkinhead was in itself a decent 80s horror ...\n49999          1  i would like to start by saying i can only hop...\n\n[50000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentiment</th>\n      <th>SentimentText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>a bit of a disappointing film, i'd say: the ac...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>the acting was terrible, the cheesy, fake, che...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>plenty has been written about mamet's \"the hou...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>\"journey to the far side of the sun\" (aka \"dop...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>i lived in that area (hoboken and jersey city)...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>1</td>\n      <td>as a big dostoyevsky fan, i had always been di...</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>0</td>\n      <td>i didn't watch this show that much when i was ...</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>1</td>\n      <td>for people who are first timers in film making...</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>0</td>\n      <td>pumpkinhead was in itself a decent 80s horror ...</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>1</td>\n      <td>i would like to start by saying i can only hop...</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "imdbData = pd.read_csv('../data/imdb_50k.tsv', sep='\\t', header=0, encoding=\"utf-8\", doublequote=False, escapechar=\"\\\\\")\n",
    "imdbData = imdbData.drop(['id'], axis=1)\n",
    "row_sizes = imdbData['SentimentText'].str.split().str.len()\n",
    "imdbData['SentimentText'] = imdbData['SentimentText'].str.lower()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "imdbData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Words count: 6365334\n",
      "                                           SentimentText\n",
      "0      bit disappointing film, i'd say: acting stilte...\n",
      "1      acting terrible, cheesy, fake, cheap green scr...\n",
      "2      plenty written mamet's \"the house games\"; good...\n",
      "3      \"journey far side sun\" (aka \"doppelganger\") en...\n",
      "4      lived area (hoboken jersey city)for ten years....\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan, always disappointed holly...\n",
      "49996  watch show much little. think watched 1 episod...\n",
      "49997  people first timers film making, think excelle...\n",
      "49998  pumpkinhead decent 80s horror flick. classic m...\n",
      "49999  would like start saying hope makers movie sist...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(imdbData, [remove_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.8876 0.8923 0.8878 0.8901 0.8881]\n",
      "Average Score: 0.88918\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                           SentimentText\n",
      "0      a bit of a disappointing film id say the actin...\n",
      "1      the acting was terrible the cheesy fake cheap ...\n",
      "2      plenty has been written about mamets the house...\n",
      "3      journey to the far side of the sun aka doppelg...\n",
      "4      i lived in that area hoboken and jersey cityfo...\n",
      "...                                                  ...\n",
      "49995  as a big dostoyevsky fan i had always been dis...\n",
      "49996  i didnt watch this show that much when i was l...\n",
      "49997  for people who are first timers in film making...\n",
      "49998  pumpkinhead was in itself a decent 80s horror ...\n",
      "49999  i would like to start by saying i can only hop...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.8847 0.885  0.8867 0.8836 0.883 ]\n",
      "Average Score: 0.8846\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(imdbData, [remove_punctuation])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Words count: 11680609\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "       Sentiment                                      SentimentText\n0              0  a bit of a disappoint film i'd say: the acting...\n1              0  the acting be terrible the cheesy fake cheap g...\n2              1  plenty have be write about mamet \"the house of...\n3              1  \"journey to the far side of the sun\" aka \"dopp...\n4              1  i live in that area hoboken and jersey city fo...\n...          ...                                                ...\n49995          1  a a big dostoyevsky fan i have always be disap...\n49996          0  i do+not watch this show that much when i be l...\n49997          1  for people who be first timer in film making i...\n49998          0  pumpkinhead be in itself a decent 80 horror fl...\n49999          1  i would like to start by say i can only hope t...\n\n[50000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentiment</th>\n      <th>SentimentText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>a bit of a disappoint film i'd say: the acting...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>the acting be terrible the cheesy fake cheap g...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>plenty have be write about mamet \"the house of...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>\"journey to the far side of the sun\" aka \"dopp...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>i live in that area hoboken and jersey city fo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>1</td>\n      <td>a a big dostoyevsky fan i have always be disap...</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>0</td>\n      <td>i do+not watch this show that much when i be l...</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>1</td>\n      <td>for people who be first timer in film making i...</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>0</td>\n      <td>pumpkinhead be in itself a decent 80 horror fl...</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>1</td>\n      <td>i would like to start by say i can only hope t...</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "imdbDataLem = pd.read_csv('../data/Imdb50KLemmatized.tsv', sep='\\t', header=0, encoding=\"utf-8\", doublequote=False, escapechar=\"\\\\\")\n",
    "imdbDataLem = imdbDataLem.drop(['id'], axis=1)\n",
    "row_sizes = imdbDataLem['SentimentText'].str.split().str.len()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "imdbDataLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                           SentimentText\n",
      "0      a bit of a disappoint film i'd say: the acting...\n",
      "1      the acting be terrible the cheesy fake cheap g...\n",
      "2      plenty have be write about mamet \"the house of...\n",
      "3      \"journey to the far side of the sun\" aka \"dopp...\n",
      "4      i live in that area hoboken and jersey city fo...\n",
      "...                                                  ...\n",
      "49995  a a big dostoyevsky fan i have always be disap...\n",
      "49996  i do+not watch this show that much when i be l...\n",
      "49997  for people who be first timer in film making i...\n",
      "49998  pumpkinhead be in itself a decent 80 horror fl...\n",
      "49999  i would like to start by say i can only hope t...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.8892 0.8849 0.8909 0.8853 0.8839]\n",
      "Average Score: 0.88684\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(imdbDataLem, [])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words AND remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                           SentimentText\n",
      "0      bit disappointing film id say acting stilted s...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty written mamets the house games good dec...\n",
      "3      journey far side sun aka doppelganger entertai...\n",
      "4      lived area hoboken jersey cityfor ten years fi...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappointed hollyw...\n",
      "49996  watch show much little think watched 1 episode...\n",
      "49997  people first timers film making think excellen...\n",
      "49998  pumpkinhead decent 80s horror flick classic me...\n",
      "49999  would like start saying hope makers movie sist...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.8914 0.8898 0.8891 0.8902 0.8866]\n",
      "Average Score: 0.8894200000000001\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(imdbData, [remove_stop_words, remove_punctuation])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words AND Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Words count: 6328911\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "[0.8939 0.893  0.8927 0.8925 0.8894]\n",
      "Average Score: 0.8923\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "scores = preprocess_train(imdbDataLem, [remove_stop_words])\n",
    "show_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-GRAMS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 2\n",
      "[0.8939 0.893  0.8927 0.8925 0.8894]\n",
      "Average Score: 0.8923\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 5\n",
      "[0.8939 0.893  0.8927 0.8925 0.8894]\n",
      "Average Score: 0.8923\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 1\n",
      "Window size: 7\n",
      "[0.8939 0.893  0.8927 0.8925 0.8894]\n",
      "Average Score: 0.8923\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 2\n",
      "Window size: 2\n",
      "[0.8979 0.8914 0.8978 0.8938 0.893 ]\n",
      "Average Score: 0.8947800000000001\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 2\n",
      "Window size: 5\n",
      "[0.8979 0.8914 0.8978 0.8938 0.893 ]\n",
      "Average Score: 0.8947800000000001\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 2\n",
      "Window size: 7\n",
      "[0.8979 0.8914 0.8978 0.8938 0.893 ]\n",
      "Average Score: 0.8947800000000001\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 3\n",
      "Window size: 2\n",
      "[0.8823 0.886  0.886  0.8819 0.8844]\n",
      "Average Score: 0.88412\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 3\n",
      "Window size: 5\n",
      "[0.8823 0.886  0.886  0.8819 0.8844]\n",
      "Average Score: 0.88412\n",
      "                                           SentimentText\n",
      "0      bit disappoint film i'd say: acting stilted so...\n",
      "1      acting terrible cheesy fake cheap green screen...\n",
      "2      plenty write mamet \"the house games\"; good dec...\n",
      "3      \"journey far side sun\" aka \"doppelganger\" ente...\n",
      "4      live area hoboken jersey city ten year film ce...\n",
      "...                                                  ...\n",
      "49995  big dostoyevsky fan always disappoint hollywoo...\n",
      "49996  do+not watch show much little think watch 1 ep...\n",
      "49997  people first timer film making think excellent...\n",
      "49998  pumpkinhead decent 80 horror flick classic mea...\n",
      "49999  would like start say hope maker movie sister f...\n",
      "\n",
      "[50000 rows x 1 columns]\n",
      "Words ngrams: 3\n",
      "Window size: 7\n",
      "[0.8823 0.886  0.886  0.8819 0.8844]\n",
      "Average Score: 0.88412\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in range(1, 4):  # word_ngrams\n",
    "    scores = preprocess_train(imdbDataLem, [remove_stop_words], word_ngrams=i)\n",
    "    show_score(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}