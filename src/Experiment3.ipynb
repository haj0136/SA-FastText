{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from skift import FirstColFtClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_score(_score):\n",
    "    print(_score)\n",
    "    print(f\"Average Score: {np.mean(_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resultsColumns = [\"index\", \"lossFunction\", \"learningRate\", \"NOiter\", \"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count: 10894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow... loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped by during the late may bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>i think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>overall i was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>the whole experience was underwhelming, and i ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>then, as if i hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         SentimentText  Sentiment\n",
       "0                             wow... loved this place.          1\n",
       "1                                   crust is not good.          0\n",
       "2            not tasty and the texture was just nasty.          0\n",
       "3    stopped by during the late may bank holiday of...          1\n",
       "4    the selection on the menu was great and so wer...          1\n",
       "..                                                 ...        ...\n",
       "995  i think food should have flavor and texture an...          0\n",
       "996                           appetite instantly gone.          0\n",
       "997  overall i was not impressed and would not go b...          0\n",
       "998  the whole experience was underwhelming, and i ...          0\n",
       "999  then, as if i hadn't wasted enough of my life ...          0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelpData = pd.read_csv('../data/yelp_labelled.txt', sep='\\t', header=0, encoding=\"utf-8\")\n",
    "row_sizes = yelpData['SentimentText'].str.split().str.len()\n",
    "yelpData['SentimentText'] = yelpData['SentimentText'].str.lower()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "yelpData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = [word for word in text.split() if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_train(data, functions, word_ngrams=1, _iterations=5, _lr=0.1, _loss=\"softmax\"):\n",
    "    _data = pd.DataFrame(data['SentimentText'])\n",
    "    for function in functions:\n",
    "        _data['SentimentText'] = _data['SentimentText'].apply(lambda x: function(x))\n",
    "    _sk_clf = FirstColFtClassifier(wordNgrams=word_ngrams, thread=1, epoch=_iterations, lr=_lr, loss=_loss)  # lr=0.3, epoch=10\n",
    "    _scores = cross_val_score(_sk_clf, _data[['SentimentText']], data['Sentiment'], cv=5, scoring='accuracy')\n",
    "    print(f\"Words ngrams: {word_ngrams}\")\n",
    "    return _scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 5, lr: 0.05\n",
      "Time: 1.1592442989349365\n",
      "[0.585 0.63  0.52  0.575 0.515]\n",
      "Average Score: 0.565\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 5, lr: 0.1\n",
      "Time: 1.1494300365447998\n",
      "[0.71  0.765 0.64  0.665 0.655]\n",
      "Average Score: 0.687\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 5, lr: 0.2\n",
      "Time: 1.1184124946594238\n",
      "[0.73  0.775 0.765 0.79  0.77 ]\n",
      "Average Score: 0.766\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 5, lr: 0.5\n",
      "Time: 1.148317575454712\n",
      "[0.71  0.76  0.755 0.795 0.75 ]\n",
      "Average Score: 0.754\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 5, lr: 1\n",
      "Time: 1.1528754234313965\n",
      "[0.71  0.745 0.76  0.79  0.73 ]\n",
      "Average Score: 0.747\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 10, lr: 0.05\n",
      "Time: 1.2479867935180664\n",
      "[0.72  0.765 0.625 0.67  0.665]\n",
      "Average Score: 0.689\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 10, lr: 0.1\n",
      "Time: 1.1389987468719482\n",
      "[0.74  0.78  0.76  0.805 0.76 ]\n",
      "Average Score: 0.7690000000000001\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 10, lr: 0.2\n",
      "Time: 1.1365158557891846\n",
      "[0.735 0.76  0.79  0.81  0.72 ]\n",
      "Average Score: 0.7630000000000001\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 10, lr: 0.5\n",
      "Time: 1.1723809242248535\n",
      "[0.735 0.735 0.775 0.8   0.735]\n",
      "Average Score: 0.756\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 10, lr: 1\n",
      "Time: 1.2061212062835693\n",
      "[0.735 0.73  0.745 0.795 0.715]\n",
      "Average Score: 0.744\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 50, lr: 0.05\n",
      "Time: 1.7650227546691895\n",
      "[0.735 0.745 0.77  0.805 0.73 ]\n",
      "Average Score: 0.757\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 50, lr: 0.1\n",
      "Time: 1.6880967617034912\n",
      "[0.735 0.715 0.77  0.805 0.745]\n",
      "Average Score: 0.754\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 50, lr: 0.2\n",
      "Time: 1.7330858707427979\n",
      "[0.71  0.715 0.75  0.77  0.74 ]\n",
      "Average Score: 0.7369999999999999\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 50, lr: 0.5\n",
      "Time: 1.6924073696136475\n",
      "[0.705 0.7   0.76  0.775 0.725]\n",
      "Average Score: 0.733\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 50, lr: 1\n",
      "Time: 1.6763203144073486\n",
      "[0.695 0.705 0.75  0.77  0.73 ]\n",
      "Average Score: 0.73\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 100, lr: 0.05\n",
      "Time: 2.728238821029663\n",
      "[0.735 0.725 0.775 0.795 0.75 ]\n",
      "Average Score: 0.756\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 100, lr: 0.1\n",
      "Time: 2.6829993724823\n",
      "[0.73  0.715 0.755 0.785 0.745]\n",
      "Average Score: 0.746\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 100, lr: 0.2\n",
      "Time: 2.7026619911193848\n",
      "[0.71  0.705 0.745 0.785 0.735]\n",
      "Average Score: 0.736\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 100, lr: 0.5\n",
      "Time: 2.6810789108276367\n",
      "[0.705 0.71  0.75  0.785 0.725]\n",
      "Average Score: 0.7350000000000001\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 100, lr: 1\n",
      "Time: 2.6975319385528564\n",
      "[0.71  0.7   0.745 0.78  0.73 ]\n",
      "Average Score: 0.7329999999999999\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 5, lr: 0.05\n",
      "Time: 1.7087557315826416\n",
      "[0.505 0.505 0.5   0.5   0.5  ]\n",
      "Average Score: 0.502\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 5, lr: 0.1\n",
      "Time: 1.7095115184783936\n",
      "[0.725 0.77  0.755 0.77  0.77 ]\n",
      "Average Score: 0.758\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 5, lr: 0.2\n",
      "Time: 1.673354148864746\n",
      "[0.715 0.77  0.755 0.815 0.765]\n",
      "Average Score: 0.764\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 5, lr: 0.5\n",
      "Time: 1.7320713996887207\n",
      "[0.725 0.755 0.76  0.785 0.735]\n",
      "Average Score: 0.752\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 5, lr: 1\n",
      "Time: 1.6862542629241943\n",
      "[0.695 0.76  0.755 0.8   0.76 ]\n",
      "Average Score: 0.7539999999999999\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 10, lr: 0.05\n",
      "Time: 1.6577210426330566\n",
      "[0.72  0.765 0.75  0.775 0.765]\n",
      "Average Score: 0.755\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 10, lr: 0.1\n",
      "Time: 1.7150516510009766\n",
      "[0.74  0.77  0.795 0.81  0.725]\n",
      "Average Score: 0.768\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 10, lr: 0.2\n",
      "Time: 1.733062744140625\n",
      "[0.715 0.73  0.78  0.805 0.735]\n",
      "Average Score: 0.7529999999999999\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 10, lr: 0.5\n",
      "Time: 1.7392098903656006\n",
      "[0.715 0.715 0.75  0.785 0.725]\n",
      "Average Score: 0.738\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 10, lr: 1\n",
      "Time: 1.769186019897461\n",
      "[0.725 0.74  0.725 0.79  0.715]\n",
      "Average Score: 0.739\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 50, lr: 0.05\n",
      "Time: 3.2855873107910156\n",
      "[0.735 0.715 0.775 0.795 0.745]\n",
      "Average Score: 0.753\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 50, lr: 0.1\n",
      "Time: 3.187600612640381\n",
      "[0.715 0.705 0.75  0.775 0.745]\n",
      "Average Score: 0.738\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 50, lr: 0.2\n",
      "Time: 2.8845877647399902\n",
      "[0.685 0.69  0.745 0.775 0.735]\n",
      "Average Score: 0.726\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 50, lr: 0.5\n",
      "Time: 2.988872766494751\n",
      "[0.695 0.685 0.755 0.76  0.73 ]\n",
      "Average Score: 0.7249999999999999\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 50, lr: 1\n",
      "Time: 3.1566240787506104\n",
      "[0.69  0.715 0.74  0.76  0.75 ]\n",
      "Average Score: 0.7309999999999999\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 100, lr: 0.05\n",
      "Time: 4.57293176651001\n",
      "[0.73  0.72  0.765 0.78  0.745]\n",
      "Average Score: 0.748\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 100, lr: 0.1\n",
      "Time: 4.889338731765747\n",
      "[0.7   0.705 0.76  0.775 0.74 ]\n",
      "Average Score: 0.736\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 100, lr: 0.2\n",
      "Time: 4.815540075302124\n",
      "[0.69  0.705 0.765 0.77  0.735]\n",
      "Average Score: 0.733\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 100, lr: 0.5\n",
      "Time: 4.323779106140137\n",
      "[0.695 0.685 0.765 0.765 0.725]\n",
      "Average Score: 0.7270000000000001\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 100, lr: 1\n",
      "Time: 4.296631097793579\n",
      "[0.69 0.7  0.76 0.78 0.74]\n",
      "Average Score: 0.734\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 5, lr: 0.05\n",
      "Time: 1.1047430038452148\n",
      "[0.54 0.58 0.51 0.63 0.51]\n",
      "Average Score: 0.554\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 5, lr: 0.1\n",
      "Time: 1.1224863529205322\n",
      "[0.61  0.655 0.56  0.7   0.535]\n",
      "Average Score: 0.6120000000000001\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 5, lr: 0.2\n",
      "Time: 1.1869499683380127\n",
      "[0.745 0.75  0.75  0.74  0.75 ]\n",
      "Average Score: 0.7470000000000001\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 5, lr: 0.5\n",
      "Time: 1.1318271160125732\n",
      "[0.72  0.76  0.755 0.82  0.76 ]\n",
      "Average Score: 0.7629999999999999\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 5, lr: 1\n",
      "Time: 1.1401946544647217\n",
      "[0.705 0.745 0.755 0.795 0.735]\n",
      "Average Score: 0.747\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 10, lr: 0.05\n",
      "Time: 1.0978329181671143\n",
      "[0.645 0.675 0.6   0.68  0.535]\n",
      "Average Score: 0.627\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 10, lr: 0.1\n",
      "Time: 1.1067922115325928\n",
      "[0.75  0.755 0.72  0.76  0.75 ]\n",
      "Average Score: 0.7469999999999999\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 10, lr: 0.2\n",
      "Time: 1.1700756549835205\n",
      "[0.745 0.775 0.795 0.805 0.735]\n",
      "Average Score: 0.771\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 10, lr: 0.5\n",
      "Time: 1.1562511920928955\n",
      "[0.72  0.735 0.77  0.805 0.73 ]\n",
      "Average Score: 0.752\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 10, lr: 1\n",
      "Time: 1.123990774154663\n",
      "[0.745 0.735 0.765 0.8   0.735]\n",
      "Average Score: 0.756\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 50, lr: 0.05\n",
      "Time: 1.6133100986480713\n",
      "[0.74  0.76  0.775 0.82  0.735]\n",
      "Average Score: 0.7659999999999999\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 50, lr: 0.1\n",
      "Time: 1.624389886856079\n",
      "[0.74  0.73  0.765 0.805 0.735]\n",
      "Average Score: 0.755\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 50, lr: 0.2\n",
      "Time: 1.6058430671691895\n",
      "[0.725 0.72  0.755 0.785 0.745]\n",
      "Average Score: 0.746\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 50, lr: 0.5\n",
      "Time: 1.6997580528259277\n",
      "[0.7   0.7   0.755 0.77  0.725]\n",
      "Average Score: 0.73\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 50, lr: 1\n",
      "Time: 1.6476342678070068\n",
      "[0.7   0.71  0.755 0.765 0.735]\n",
      "Average Score: 0.733\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 100, lr: 0.05\n",
      "Time: 2.212423086166382\n",
      "[0.74  0.74  0.78  0.79  0.735]\n",
      "Average Score: 0.7569999999999999\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 100, lr: 0.1\n",
      "Time: 2.2288033962249756\n",
      "[0.745 0.72  0.765 0.8   0.75 ]\n",
      "Average Score: 0.756\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 100, lr: 0.2\n",
      "Time: 2.2246458530426025\n",
      "[0.73 0.71 0.75 0.79 0.74]\n",
      "Average Score: 0.744\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 100, lr: 0.5\n",
      "Time: 2.1703412532806396\n",
      "[0.725 0.7   0.745 0.785 0.725]\n",
      "Average Score: 0.736\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 100, lr: 1\n",
      "Time: 2.199216604232788\n",
      "[0.71  0.71  0.765 0.785 0.725]\n",
      "Average Score: 0.7390000000000001\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "data = []\n",
    "for loss in [\"softmax\", \"ns\", \"hs\"]: \n",
    "    for epoch in [5, 10, 50 ,100]:\n",
    "        for lr in [0.05, 0.1, 0.2, 0.5, 1]:\n",
    "            start = time.time()\n",
    "            ## Removed stop words AND removed punctuation\n",
    "            scores = preprocess_train(yelpData, [remove_stop_words, remove_punctuation], _iterations=epoch, _lr=lr, _loss=loss)\n",
    "            end = time.time()\n",
    "            print(f\"Loss: {loss}, epoch: {epoch}, lr: {lr}\")\n",
    "            print(f\"Time: {end - start}\")\n",
    "            show_score(scores)\n",
    "            data.append([index, loss, lr, epoch, np.mean(scores)])\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 200, lr: 0.05\n",
      "Time: 4.032491445541382\n",
      "[0.745 0.73  0.78  0.785 0.735]\n",
      "Average Score: 0.755\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 200, lr: 0.1\n",
      "Time: 3.8946728706359863\n",
      "[0.725 0.72  0.755 0.785 0.75 ]\n",
      "Average Score: 0.747\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 200, lr: 0.2\n",
      "Time: 3.765923261642456\n",
      "[0.71  0.7   0.745 0.79  0.735]\n",
      "Average Score: 0.736\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 200, lr: 0.5\n",
      "Time: 3.6832127571105957\n",
      "[0.715 0.7   0.74  0.79  0.725]\n",
      "Average Score: 0.7340000000000001\n",
      "Words ngrams: 1\n",
      "Loss: softmax, epoch: 200, lr: 1\n",
      "Time: 3.7989680767059326\n",
      "[0.715 0.71  0.75  0.785 0.74 ]\n",
      "Average Score: 0.74\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 200, lr: 0.05\n",
      "Time: 6.860926151275635\n",
      "[0.725 0.73  0.76  0.785 0.745]\n",
      "Average Score: 0.749\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 200, lr: 0.1\n",
      "Time: 6.745258331298828\n",
      "[0.69 0.71 0.76 0.78 0.73]\n",
      "Average Score: 0.7340000000000001\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 200, lr: 0.2\n",
      "Time: 6.827725172042847\n",
      "[0.685 0.695 0.765 0.785 0.73 ]\n",
      "Average Score: 0.732\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 200, lr: 0.5\n",
      "Time: 6.734008073806763\n",
      "[0.695 0.695 0.75  0.78  0.745]\n",
      "Average Score: 0.733\n",
      "Words ngrams: 1\n",
      "Loss: ns, epoch: 200, lr: 1\n",
      "Time: 6.7190141677856445\n",
      "[0.715 0.715 0.74  0.785 0.735]\n",
      "Average Score: 0.738\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 200, lr: 0.05\n",
      "Time: 3.7181527614593506\n",
      "[0.735 0.725 0.79  0.775 0.74 ]\n",
      "Average Score: 0.7529999999999999\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 200, lr: 0.1\n",
      "Time: 3.775981903076172\n",
      "[0.75  0.725 0.765 0.785 0.74 ]\n",
      "Average Score: 0.7530000000000001\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 200, lr: 0.2\n",
      "Time: 3.749061346054077\n",
      "[0.72  0.72  0.755 0.785 0.745]\n",
      "Average Score: 0.745\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 200, lr: 0.5\n",
      "Time: 3.638434410095215\n",
      "[0.72  0.705 0.745 0.79  0.725]\n",
      "Average Score: 0.737\n",
      "Words ngrams: 1\n",
      "Loss: hs, epoch: 200, lr: 1\n",
      "Time: 3.7469403743743896\n",
      "[0.71  0.705 0.76  0.79  0.72 ]\n",
      "Average Score: 0.7369999999999999\n"
     ]
    }
   ],
   "source": [
    "# epoch 200 only\n",
    "for loss in [\"softmax\", \"ns\", \"hs\"]:\n",
    "    for epoch in [200]:\n",
    "        for lr in [0.05, 0.1, 0.2, 0.5, 1]:\n",
    "            start = time.time()\n",
    "            scores = preprocess_train(yelpData, [remove_stop_words, remove_punctuation], _iterations=epoch, _lr=lr, _loss=loss)\n",
    "            end = time.time()\n",
    "            print(f\"Loss: {loss}, epoch: {epoch}, lr: {lr}\")\n",
    "            print(f\"Time: {end - start}\")\n",
    "            show_score(scores)\n",
    "            data.append([index, loss, lr, epoch, np.mean(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data, columns=resultsColumns)\n",
    "results_df.to_csv(\"Ex3FastText/Ex3ReportYelp.csv\", index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count: 11680609\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a bit of a disappoint film i'd say: the acting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the acting be terrible the cheesy fake cheap g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>plenty have be write about mamet \"the house of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>\"journey to the far side of the sun\" aka \"dopp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i live in that area hoboken and jersey city fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>1</td>\n",
       "      <td>a a big dostoyevsky fan i have always be disap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0</td>\n",
       "      <td>i do+not watch this show that much when i be l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>1</td>\n",
       "      <td>for people who be first timer in film making i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0</td>\n",
       "      <td>pumpkinhead be in itself a decent 80 horror fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1</td>\n",
       "      <td>i would like to start by say i can only hope t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                      SentimentText\n",
       "0              0  a bit of a disappoint film i'd say: the acting...\n",
       "1              0  the acting be terrible the cheesy fake cheap g...\n",
       "2              1  plenty have be write about mamet \"the house of...\n",
       "3              1  \"journey to the far side of the sun\" aka \"dopp...\n",
       "4              1  i live in that area hoboken and jersey city fo...\n",
       "...          ...                                                ...\n",
       "49995          1  a a big dostoyevsky fan i have always be disap...\n",
       "49996          0  i do+not watch this show that much when i be l...\n",
       "49997          1  for people who be first timer in film making i...\n",
       "49998          0  pumpkinhead be in itself a decent 80 horror fl...\n",
       "49999          1  i would like to start by say i can only hope t...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbDataLem = pd.read_csv('../data/Imdb50KLemmatized.tsv', sep='\\t', header=0, encoding=\"utf-8\", doublequote=False, escapechar=\"\\\\\")\n",
    "imdbDataLem = imdbDataLem.drop(['id'], axis=1)\n",
    "row_sizes = imdbDataLem['SentimentText'].str.split().str.len()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "imdbDataLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 5, lr: 0.05\n",
      "Time: 184.29815816879272\n",
      "[0.8646 0.872  0.8727 0.865  0.87  ]\n",
      "Average Score: 0.8688600000000001\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 5, lr: 0.1\n",
      "Time: 181.4628188610077\n",
      "[0.8979 0.8914 0.8978 0.8938 0.893 ]\n",
      "Average Score: 0.8947800000000001\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 5, lr: 0.2\n",
      "Time: 184.90914583206177\n",
      "[0.9062 0.8985 0.9051 0.8994 0.9022]\n",
      "Average Score: 0.90228\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 5, lr: 0.5\n",
      "Time: 176.4304177761078\n",
      "[0.9068 0.9013 0.9057 0.9019 0.9032]\n",
      "Average Score: 0.90378\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 5, lr: 1\n",
      "Time: 166.16732001304626\n",
      "[0.9072 0.902  0.9041 0.9006 0.9014]\n",
      "Average Score: 0.90306\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 10, lr: 0.05\n",
      "Time: 293.83214044570923\n",
      "[0.8981 0.8939 0.898  0.8946 0.8928]\n",
      "Average Score: 0.89548\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 10, lr: 0.1\n",
      "Time: 328.14668583869934\n",
      "[0.9069 0.9    0.906  0.9002 0.9042]\n",
      "Average Score: 0.9034600000000002\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 10, lr: 0.2\n",
      "Time: 360.3794515132904\n",
      "[0.9071 0.9025 0.9074 0.9019 0.9058]\n",
      "Average Score: 0.9049400000000001\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 10, lr: 0.5\n",
      "Time: 357.1498324871063\n",
      "[0.9062 0.9038 0.9064 0.9011 0.9039]\n",
      "Average Score: 0.90428\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 10, lr: 1\n",
      "Time: 357.9929475784302\n",
      "[0.9072 0.9012 0.9052 0.901  0.9012]\n",
      "Average Score: 0.9031600000000001\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 50, lr: 0.05\n",
      "Time: 1495.8112425804138\n",
      "[0.9065 0.9037 0.9075 0.9034 0.9056]\n",
      "Average Score: 0.90534\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 50, lr: 0.1\n",
      "Time: 1498.9541761875153\n",
      "[0.9075 0.9045 0.9086 0.9036 0.9057]\n",
      "Average Score: 0.9059799999999999\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 50, lr: 0.2\n",
      "Time: 1418.2641665935516\n",
      "[0.9065 0.9047 0.9079 0.9033 0.9051]\n",
      "Average Score: 0.9055\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 50, lr: 0.5\n",
      "Time: 1302.0360605716705\n",
      "[0.9076 0.9045 0.908  0.9019 0.9048]\n",
      "Average Score: 0.9053599999999999\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 50, lr: 1\n",
      "Time: 1268.6156044006348\n",
      "[0.9022 0.8991 0.9035 0.8998 0.9015]\n",
      "Average Score: 0.90122\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 100, lr: 0.05\n",
      "Time: 2214.154520750046\n",
      "[0.9074 0.9045 0.909  0.903  0.9053]\n",
      "Average Score: 0.9058400000000001\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 100, lr: 0.1\n",
      "Time: 2216.238471031189\n",
      "[0.9071 0.9042 0.9092 0.903  0.9055]\n",
      "Average Score: 0.9057999999999999\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 100, lr: 0.2\n",
      "Time: 2241.367175579071\n",
      "[0.9066 0.905  0.9087 0.9032 0.9053]\n",
      "Average Score: 0.9057600000000001\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 100, lr: 0.5\n",
      "Time: 2243.827865600586\n",
      "[0.9072 0.9042 0.9076 0.9024 0.9046]\n",
      "Average Score: 0.9052\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 100, lr: 1\n",
      "Time: 2204.7162775993347\n",
      "[0.9054 0.9016 0.9042 0.8982 0.9005]\n",
      "Average Score: 0.90198\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 5, lr: 0.05\n",
      "Time: 164.94618487358093\n",
      "[0.8954 0.8913 0.8956 0.8921 0.8911]\n",
      "Average Score: 0.8930999999999999\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 5, lr: 0.1\n",
      "Time: 164.38075470924377\n",
      "[0.9055 0.8979 0.9051 0.8998 0.9018]\n",
      "Average Score: 0.9020199999999999\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 5, lr: 0.2\n",
      "Time: 164.04248237609863\n",
      "[0.907  0.9017 0.9074 0.9002 0.9038]\n",
      "Average Score: 0.90402\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 5, lr: 0.5\n",
      "Time: 164.1774501800537\n",
      "[0.9091 0.9014 0.9047 0.8994 0.9017]\n",
      "Average Score: 0.9032599999999998\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 5, lr: 1\n",
      "Time: 164.16599440574646\n",
      "[0.9082 0.8982 0.9005 0.9001 0.8985]\n",
      "Average Score: 0.9011000000000001\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 10, lr: 0.05\n",
      "Time: 274.9634368419647\n",
      "[0.9059 0.8996 0.9067 0.8998 0.9033]\n",
      "Average Score: 0.90306\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 10, lr: 0.1\n",
      "Time: 273.552969455719\n",
      "[0.9076 0.9022 0.9067 0.9017 0.906 ]\n",
      "Average Score: 0.9048399999999999\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 10, lr: 0.2\n",
      "Time: 272.91919803619385\n",
      "[0.9081 0.9039 0.9076 0.9007 0.9047]\n",
      "Average Score: 0.9049999999999999\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 10, lr: 0.5\n",
      "Time: 273.20433044433594\n",
      "[0.9083 0.9011 0.9034 0.9012 0.9009]\n",
      "Average Score: 0.90298\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 10, lr: 1\n",
      "Time: 274.10038685798645\n",
      "[0.9053 0.8995 0.9037 0.8988 0.8995]\n",
      "Average Score: 0.90136\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 50, lr: 0.05\n",
      "Time: 1154.0804262161255\n",
      "[0.9074 0.9045 0.9084 0.9037 0.9057]\n",
      "Average Score: 0.90594\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 50, lr: 0.1\n",
      "Time: 1146.1493802070618\n",
      "[0.9064 0.9048 0.9079 0.9031 0.9049]\n",
      "Average Score: 0.9054200000000001\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 50, lr: 0.2\n",
      "Time: 1147.292137145996\n",
      "[0.9072 0.9047 0.9077 0.9039 0.9046]\n",
      "Average Score: 0.9056200000000001\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 50, lr: 0.5\n",
      "Time: 1150.1046168804169\n",
      "[0.9024 0.9033 0.9046 0.8994 0.9005]\n",
      "Average Score: 0.9020399999999998\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 50, lr: 1\n",
      "Time: 1146.8402087688446\n",
      "[0.901  0.8995 0.9047 0.8962 0.8988]\n",
      "Average Score: 0.90004\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 100, lr: 0.05\n",
      "Time: 2244.548224210739\n",
      "[0.9071 0.9048 0.9088 0.9028 0.9059]\n",
      "Average Score: 0.90588\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 100, lr: 0.1\n",
      "Time: 2256.3404681682587\n",
      "[0.9063 0.9047 0.9084 0.9033 0.9058]\n",
      "Average Score: 0.9057000000000001\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 100, lr: 0.2\n",
      "Time: 2265.6202170848846\n",
      "[0.9067 0.9055 0.9076 0.9033 0.9045]\n",
      "Average Score: 0.9055199999999999\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 100, lr: 0.5\n",
      "Time: 2829.041135787964\n",
      "[0.9047 0.9    0.9034 0.899  0.8995]\n",
      "Average Score: 0.9013199999999999\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 100, lr: 1\n",
      "Time: 2308.734627723694\n",
      "[0.9015 0.8982 0.9029 0.8967 0.8999]\n",
      "Average Score: 0.89984\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 5, lr: 0.05\n",
      "Time: 160.69027972221375\n",
      "[0.825  0.8351 0.8396 0.8311 0.8328]\n",
      "Average Score: 0.8327199999999999\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 5, lr: 0.1\n",
      "Time: 159.8931918144226\n",
      "[0.8838 0.8869 0.888  0.8825 0.8855]\n",
      "Average Score: 0.88534\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 5, lr: 0.2\n",
      "Time: 161.47496271133423\n",
      "[0.903  0.8974 0.9022 0.8998 0.8987]\n",
      "Average Score: 0.90022\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 5, lr: 0.5\n",
      "Time: 163.9054582118988\n",
      "[0.9064 0.9007 0.9069 0.9007 0.9049]\n",
      "Average Score: 0.9039200000000001\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 5, lr: 1\n",
      "Time: 158.36433911323547\n",
      "[0.9076 0.9023 0.9055 0.8999 0.903 ]\n",
      "Average Score: 0.90366\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 10, lr: 0.05\n",
      "Time: 265.8931152820587\n",
      "[0.8838 0.8874 0.8881 0.8834 0.8856]\n",
      "Average Score: 0.88566\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 10, lr: 0.1\n",
      "Time: 264.92357778549194\n",
      "[0.9029 0.8978 0.9021 0.8998 0.8998]\n",
      "Average Score: 0.90048\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 10, lr: 0.2\n",
      "Time: 266.09273409843445\n",
      "[0.9067 0.9013 0.9064 0.9015 0.9052]\n",
      "Average Score: 0.9042199999999999\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 10, lr: 0.5\n",
      "Time: 265.7949962615967\n",
      "[0.9077 0.9039 0.9078 0.901  0.9056]\n",
      "Average Score: 0.9052\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 10, lr: 1\n",
      "Time: 265.93759298324585\n",
      "[0.9063 0.902  0.9059 0.9021 0.9038]\n",
      "Average Score: 0.90402\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 50, lr: 0.05\n",
      "Time: 1149.177568435669\n",
      "[0.9064 0.9033 0.9082 0.903  0.9052]\n",
      "Average Score: 0.9052199999999999\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 50, lr: 0.1\n",
      "Time: 1229.3966381549835\n",
      "[0.9072 0.9042 0.9083 0.903  0.9055]\n",
      "Average Score: 0.90564\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 50, lr: 0.2\n",
      "Time: 1219.4205160140991\n",
      "[0.9072 0.9047 0.9085 0.9031 0.9054]\n",
      "Average Score: 0.90578\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 50, lr: 0.5\n",
      "Time: 1160.928120136261\n",
      "[0.9072 0.9044 0.9084 0.9026 0.905 ]\n",
      "Average Score: 0.9055199999999999\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 50, lr: 1\n",
      "Time: 1127.5613868236542\n",
      "[0.9056 0.9016 0.907  0.9021 0.9052]\n",
      "Average Score: 0.9042999999999999\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 100, lr: 0.05\n",
      "Time: 2175.8117253780365\n",
      "[0.9065 0.9046 0.9082 0.9026 0.9054]\n",
      "Average Score: 0.90546\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 100, lr: 0.1\n",
      "Time: 2187.0077521800995\n",
      "[0.9072 0.9046 0.9092 0.9032 0.9055]\n",
      "Average Score: 0.90594\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 100, lr: 0.2\n",
      "Time: 2403.7321331501007\n",
      "[0.9069 0.9048 0.909  0.903  0.9053]\n",
      "Average Score: 0.9057999999999999\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 100, lr: 0.5\n",
      "Time: 2357.8954474925995\n",
      "[0.9066 0.9045 0.909  0.902  0.9051]\n",
      "Average Score: 0.9054400000000001\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 100, lr: 1\n",
      "Time: 2300.1276516914368\n",
      "[0.9058 0.9028 0.9063 0.9009 0.9039]\n",
      "Average Score: 0.9039400000000001\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "data = []\n",
    "for loss in [\"softmax\", \"ns\", \"hs\"]:  # \"softmax\", \"ns\", \"hs\"\n",
    "    for epoch in [5, 10, 50 ,100]:\n",
    "        for lr in [0.05, 0.1, 0.2, 0.5, 1]:\n",
    "            start = time.time()\n",
    "            scores = preprocess_train(imdbDataLem, [remove_stop_words], _iterations=epoch, _lr=lr, _loss=loss, word_ngrams=2)\n",
    "            end = time.time()\n",
    "            print(f\"Loss: {loss}, epoch: {epoch}, lr: {lr}\")\n",
    "            print(f\"Time: {end - start}\")\n",
    "            show_score(scores)\n",
    "            data.append([index, loss, lr, epoch, np.mean(scores)])\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 200, lr: 0.05\n",
      "Time: 4371.943948030472\n",
      "[0.9071 0.9049 0.9095 0.9033 0.9051]\n",
      "Average Score: 0.9059799999999999\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 200, lr: 0.1\n",
      "Time: 4313.324980020523\n",
      "[0.9068 0.9046 0.9091 0.9029 0.9052]\n",
      "Average Score: 0.90572\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 200, lr: 0.2\n",
      "Time: 4589.946355104446\n",
      "[0.9065 0.9046 0.9089 0.9029 0.9048]\n",
      "Average Score: 0.9055399999999999\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 200, lr: 0.5\n",
      "Time: 5762.751471996307\n",
      "[0.9067 0.9041 0.9074 0.9023 0.9051]\n",
      "Average Score: 0.9051199999999999\n",
      "Words ngrams: 2\n",
      "Loss: softmax, epoch: 200, lr: 1\n",
      "Time: 5625.8195769786835\n",
      "[0.9039 0.9018 0.9046 0.8994 0.9017]\n",
      "Average Score: 0.90228\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 200, lr: 0.05\n",
      "Time: 6980.918215513229\n",
      "[0.9067 0.9046 0.909  0.9031 0.9052]\n",
      "Average Score: 0.90572\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 200, lr: 0.1\n",
      "Time: 4303.546540975571\n",
      "[0.9064 0.9042 0.9084 0.9025 0.9045]\n",
      "Average Score: 0.9052\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 200, lr: 0.2\n",
      "Time: 4281.647273540497\n",
      "[0.9063 0.9052 0.9083 0.9028 0.9036]\n",
      "Average Score: 0.90524\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 200, lr: 0.5\n",
      "Time: 4253.226505994797\n",
      "[0.9033 0.9017 0.901  0.9    0.899 ]\n",
      "Average Score: 0.901\n",
      "Words ngrams: 2\n",
      "Loss: ns, epoch: 200, lr: 1\n",
      "Time: 4292.535616636276\n",
      "[0.9014 0.8967 0.9034 0.8989 0.8987]\n",
      "Average Score: 0.8998200000000001\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 200, lr: 0.05\n",
      "Time: 4131.655542850494\n",
      "[0.9072 0.9048 0.9088 0.9033 0.9052]\n",
      "Average Score: 0.90586\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 200, lr: 0.1\n",
      "Time: 4139.852431058884\n",
      "[0.9069 0.9046 0.9094 0.9028 0.9051]\n",
      "Average Score: 0.9057600000000001\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 200, lr: 0.2\n",
      "Time: 4185.619000911713\n",
      "[0.9068 0.9048 0.9091 0.9026 0.9047]\n",
      "Average Score: 0.9056\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 200, lr: 0.5\n",
      "Time: 4484.341857433319\n",
      "[0.9065 0.9048 0.9079 0.9023 0.9047]\n",
      "Average Score: 0.90524\n",
      "Words ngrams: 2\n",
      "Loss: hs, epoch: 200, lr: 1\n",
      "Time: 4422.839933395386\n",
      "[0.902  0.8986 0.9045 0.8977 0.9031]\n",
      "Average Score: 0.9011799999999999\n"
     ]
    }
   ],
   "source": [
    "# epoch 200 only\n",
    "for loss in [\"softmax\", \"ns\", \"hs\"]:  # \"softmax\", \"ns\", \"hs\"\n",
    "    for epoch in [200]:\n",
    "        for lr in [0.05, 0.1, 0.2, 0.5, 1]:\n",
    "            start = time.time()\n",
    "            scores = preprocess_train(imdbDataLem, [remove_stop_words], _iterations=epoch, _lr=lr, _loss=loss, word_ngrams=2)\n",
    "            end = time.time()\n",
    "            print(f\"Loss: {loss}, epoch: {epoch}, lr: {lr}\")\n",
    "            print(f\"Time: {end - start}\")\n",
    "            show_score(scores)\n",
    "            data.append([index, loss, lr, epoch, np.mean(scores)])\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data, columns=resultsColumns)\n",
    "results_df.to_csv(\"../data/Ex3FastText/Ex3ReportImdb.csv\", index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}